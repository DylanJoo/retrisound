---
config_file: configs/tinyllama-likelihood.yaml
retriever_name_or_path: facebook/contriever-msmarco
generator_name_or_path: TinyLlama/TinyLlama-1.1B-Chat-v0.6
attn_implementation: flash_attention_2
train_file: /home/dju/datasets/asqa/ASQA.json
corpus_file: /home/dju/datasets/wikipedia_split/
retrieval_file: /home/dju/datasets/asqa/train_data_bm25-top100.run
dataloader_num_workers: 16
learning_rate: 0.00001
lr_scheduler_type: linear
warmup_ratio: 0.1
weight_decay: 0.
report_to: wandb
output_dir: /ivi/ilps/personal/dju/checkpoints/adarag_1B
num_train_epochs: 10
save_steps: 1000
logging_steps: 1
total_episodes: 10000
num_ppo_epochs: 10
n_contexts: 10
n_max_candidates: 30
depth: 30
n_max_segments: 5
num_steps: 5
reward_function: likelihood
cont_coef: 0.0
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
